{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CDY6oGNV4nz"
   },
   "source": [
    "###                  ADAS VIDEO PERCEPTION → RAW INSTRUCTION DATASET PIPELINE\n",
    "###     Qwen2.5-VL | Automatic Scene Parsing | JSON Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4LHAgDPWI52"
   },
   "source": [
    "This script processes a directory of driving videos and automatically generates a raw instruction dataset for ADAS/Autonomous Driving applications using a custom vision-language model.\n",
    "\n",
    "For each video, the system produces structured scene understanding, driving parameters, and risk assessments in natural language and JSON format. The generated outputs serve as initial (raw) instruction data that can later be refined, cleaned, or validated using domain-specific rules, human annotation, or additional QA tools. This pipeline enables scalable, automated creation of training data for perception, reasoning, and safety-related ADAS models.\n",
    "\n",
    "Author: Sarvesh Telang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t__dQr-Y4h-"
   },
   "source": [
    "The default configuration uses:\n",
    "\n",
    "**Qwen2.5-VL-3B-Instruct** — a compact vision–language model suitable for image/ video understanding and supported on colab free tier setting\n",
    "\n",
    "The `MODEL_NAME` field can be replaced with other supported Hugging Face models depending on the application. Examples include:\n",
    "\n",
    "• Video Understanding Models such as **Video-LLaMA** for multi-frame and temporal reasoning `\"DAMO-NLP-SG/Video-LLaMA-2-7B\"`\n",
    "\n",
    "• Object Detection Models such as the **YOLO series** (YOLOv8, YOLO-NAS) for detection tasks (using `ultralytics`)\n",
    "\n",
    "• General-Purpose VLM Models such as **LLaVA** or **LLaVA-NeXT** for image–text reasoning `\"llava-hf/llava-v1.6-vicuna-13b-hf\"`\n",
    "\n",
    "• Grounding and Captioning Models such as **InternVL2** or **Florence-2** `\"OpenGVLab/InternVL2-8B\"`\n",
    "\n",
    "• Captioning / VQA Models such as **BLIP** / **BLIP-2** `\"Salesforce/blip2-opt-2.7b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 1. MOUNT GOOGLE DRIVE\n",
    "# ---------------------------------------------------------------\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "VIDEO_DIR = \"/content/drive/MyDrive/SFT_VLA_dataset\" # video folder path\n",
    "SAVE_PATH = \"/content/drive/MyDrive\"\n",
    "OUTPUT_JSON = f\"{SAVE_PATH}/BDD_instruct_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB88ChyDWZgr"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 2. INSTALL DEPENDENCIES\n",
    "# ---------------------------------------------------------------\n",
    "!pip install -q git+https://github.com/huggingface/transformers accelerate\n",
    "!pip install -q qwen-vl-utils[decord]==0.0.8   # video support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-Bk97jyWdZk"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 3. IMPORTS & MODEL LOADING\n",
    "# ---------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\" # Select custom model for raw instruction generation\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OE9mIkC1XcVN"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 4. PROMPT TEMPLATE\n",
    "# ---------------------------------------------------------------\n",
    "PROMPT = \"\"\"\n",
    "You are an ADAS driving assistant. Analyze the scene from a driving and safety perspective, and produce Scene Description, Driving Parameters and Risk Assessment. Follow below instructions:\n",
    "\n",
    "1. Scene Description:\n",
    "   - Focus only on elements relevant to driving behavior and safety.\n",
    "   - Describe the positions, movements, and actions of other vehicles, pedestrians, and obstacles.\n",
    "   - Mention traffic signs, lights, road markings, and lane information if relevant.\n",
    "   - Highlight any potential hazards or situations that require attention from the ego vehicle.\n",
    "\n",
    "2. Driving Parameters (JSON):\n",
    "{\n",
    " \"road_type\": \"...\",\n",
    " \"lane_count\": \"...\",\n",
    " \"ego_lane_position\": \"...\",\n",
    " \"traffic_light_state\": \"...\",\n",
    " \"pedestrian_on_road\": \"...\",\n",
    " \"closest_vehicle_distance\": \"...\",\n",
    " \"ego_vehicle_speed\": \"...\",\n",
    " \"road_curvature\": \"...\",\n",
    " \"weather\": \"...\",\n",
    " \"visibility\": \"...\",\n",
    " \"traffic_density\": \"...\",\n",
    " \"risk_factor\": \"...\"\n",
    "}\n",
    "\n",
    "3. Risk Assessment:\n",
    "   - Consider nearby vehicles, pedestrians, road conditions, traffic rules, visibility and environmental conditions.\n",
    "\n",
    "Do not mention that it is a video or footage. Provide precise, actionable observations for an ADAS system.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsh_qlo8ZquD"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 5. PROCESS ALL VIDEOS AND GENERATE RESPONSES\n",
    "# ---------------------------------------------------------------\n",
    "video_files = sorted(\n",
    "    f for f in os.listdir(VIDEO_DIR)\n",
    "    if f.lower().endswith((\".mp4\", \".mov\", \".avi\"))\n",
    ")\n",
    "\n",
    "results = []\n",
    "start_total = time.time()\n",
    "\n",
    "for video_name in tqdm(video_files, desc=\"Processing videos\", unit=\"video\"):\n",
    "\n",
    "    video_path = os.path.join(VIDEO_DIR, video_name)\n",
    "    print(f\"\\n▶ Processing {video_name}\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": video_path,\n",
    "                    \"max_pixels\": 360 * 420,\n",
    "                    \"fps\": 1.0\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": ADAS_PROMPT},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Encode text + video\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "    trimmed_ids = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output = processor.batch_decode(\n",
    "        trimmed_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "\n",
    "    # Save entry\n",
    "    results.append({\n",
    "        \"video_id\": video_name,\n",
    "        \"QA\": {\"q\": PROMPT, \"a\": output}\n",
    "    })\n",
    "\n",
    "# Save all responses\n",
    "with open(OUTPUT_JSON, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\n✅ Saved output to:\", OUTPUT_JSON)\n",
    "print(f\"⏱ Total processing time: {time.time()-start_total:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiaEBrkxZwVi"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 6. VALIDATION (OPTIONAL): TO ENSURE CONSISTENT COMPLETION FORMAT\n",
    "# ===============================================================\n",
    "import re\n",
    "\n",
    "RE_JSON_BLOCK = re.compile(r\"```json(.*?)```\", re.DOTALL)\n",
    "\n",
    "REQUIRED_FIELDS = {\n",
    "    \"road_type\", \"lane_count\", \"ego_lane_position\", \"traffic_light_state\",\n",
    "    \"pedestrian_on_road\", \"closest_vehicle_distance\", \"ego_vehicle_speed\",\n",
    "    \"road_curvature\", \"weather\", \"visibility\", \"traffic_density\", \"risk_factor\"\n",
    "}\n",
    "\n",
    "def find_line(all_lines, text):\n",
    "    for i, line in enumerate(all_lines, start=1):\n",
    "        if text in line:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "# ----------------------- JSON Error Context --------------------\n",
    "def json_error_context(json_text):\n",
    "    try:\n",
    "        json.loads(json_text)\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        lines = json_text.split(\"\\n\")\n",
    "        faulty = lines[e.lineno-1]\n",
    "        caret = \" \" * (e.colno-1) + \"^\"\n",
    "        return (e.msg, e.lineno, e.colno, faulty, caret)\n",
    "\n",
    "\n",
    "# ----------------------- Entry Validation ----------------------\n",
    "def validate_entry(entry, all_lines):\n",
    "    errors = []\n",
    "    answer = entry[\"QA\"][\"a\"]\n",
    "\n",
    "    # Check sections\n",
    "    for section in [\"Scene Description\", \"Driving Parameters\", \"Risk Assessment\"]:\n",
    "        if section not in answer:\n",
    "            errors.append((f\"Missing section: {section}\", None))\n",
    "\n",
    "    # Extract JSON block\n",
    "    match = RE_JSON_BLOCK.search(answer)\n",
    "    if not match:\n",
    "        errors.append((\"Missing JSON block\", None))\n",
    "        return errors\n",
    "\n",
    "    json_text = match.group(1).strip()\n",
    "    json_line = find_line(all_lines, json_text.split(\"\\n\")[0])\n",
    "\n",
    "    # JSON validity check\n",
    "    context = json_error_context(json_text)\n",
    "    if context:\n",
    "        msg, line, col, faulty, caret = context\n",
    "        errors.append((f\"Invalid JSON: {msg}\", json_line + line - 1, faulty, caret))\n",
    "        return errors\n",
    "\n",
    "    # Structure validation\n",
    "    parsed = json.loads(json_text)\n",
    "    missing = REQUIRED_FIELDS - set(parsed.keys())\n",
    "    extra = set(parsed.keys()) - REQUIRED_FIELDS\n",
    "\n",
    "    if missing:\n",
    "        errors.append((f\"Missing JSON fields: {missing}\", json_line))\n",
    "    if extra:\n",
    "        errors.append((f\"Extra JSON fields: {extra}\", json_line))\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# ----------------------- Full File Validation ------------------\n",
    "def validate_file(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        all_lines = f.readlines()\n",
    "\n",
    "    dataset = json.loads(\"\".join(all_lines\"))\n",
    "    print(\"\\n==================== VALIDATION REPORT ====================\\n\")\n",
    "\n",
    "    for item in dataset:\n",
    "        video_id = item[\"video_id\"]\n",
    "        issues = validate_entry(item, all_lines)\n",
    "\n",
    "        if not issues:\n",
    "            print(f\"[✔ OK] {video_id}\")\n",
    "        else:\n",
    "            print(f\"[❌ Issues in] {video_id}:\")\n",
    "            for e in issues:\n",
    "                if len(e) == 4:\n",
    "                    msg, line, faulty, caret = e\n",
    "                    print(f\"   Line {line}: {msg}\")\n",
    "                    print(f\"      {faulty}\")\n",
    "                    print(f\"      {caret}\")\n",
    "                else:\n",
    "                    print(f\"   {e[0]}\")\n",
    "\n",
    "    print(\"\\n========================= DONE =============================\\n\")\n",
    "\n",
    "\n",
    "# ----------------------- Run Validation ------------------------\n",
    "validate_file(OUTPUT_JSON)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBFIqOQNFlYEWDzp6Y4NfS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
