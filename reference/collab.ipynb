{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT finetuning of the Qwen3-VL 4B-Instruct model on a custom multi-image vision–language dataset, using QLoRA 4-bit quantization and TRL’s SFTTrainer\n",
    "\n",
    "\n",
    "Designed to run on a Google Colab free-tier T4 GPU, the script showcases a resource-efficient strategy for finetuning large vision–language models for video understanding. It covers the full workflow, including dataset loading, model and PEFT adapter configuration, training, and an inference pipeline that extracts frames from a video and uses the finetuned model to answer video-related questions.\n",
    "\n",
    "Instead of processing full video sequences—which would normally require FlashAttention 2, supports only A100/Hopper GPUs (Not Turing GPUs) —the approach extracts a fixed number of frames per video, creating multi-image inputs that are compatible with T4 GPUs while still enabling meaningful temporal reasoning.\n",
    "\n",
    "The project uses the custom dataset \"SarveshBTelang/SFT_VLA_Dataset_1.0\" hosted on the Hugging Face Hub. Each record contains multiple images along with instruction–completion text pairs for supervised finetuning. Images are sourced from BDD100K driving videos, and paired with instructions from https://github.com/sungyeonparkk/vision-assistant-for-driving\n",
    ".\n",
    "The codebase is modular and can be adapted to other VL models or image datasets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# === Imports ===\n",
    "import os\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_info()\n",
    "\n",
    "# For Qwen3-VL model class, depending on the repo you might need trust_remote_code=True when loading.\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Optional: cv2 and PIL for frame extraction\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 1) Login to Hugging Face (interactive in Colab) ===\n",
    "# In Colab you'll be prompted for a token. Locally ensure `huggingface-cli login` is done.\n",
    "\n",
    "\n",
    "# === 2) Load dataset ===\n",
    "# Replace with your dataset path on the Hub or local dataset.\n",
    "DATASET_ID = \"SarveshBTelang/SFT_VLA_Dataset_1.0\"\n",
    "print(\"Loading dataset:\", DATASET_ID)\n",
    "train_dataset = load_dataset(DATASET_ID, split=\"train\")\n",
    "\n",
    "# Utility to ensure prompt/completion fields are lists (compatible with trainer code below)\n",
    "def fix_dataset_format(hf_dataset):\n",
    "    \"\"\"Ensure prompt and completion are lists. This works with Dataset.map (batched=False).\"\"\"\n",
    "    if isinstance(hf_dataset.get(\"prompt\"), dict):\n",
    "        hf_dataset[\"prompt\"] = [hf_dataset[\"prompt\"]]\n",
    "    if isinstance(hf_dataset.get(\"completion\"), dict):\n",
    "        hf_dataset[\"completion\"] = [hf_dataset[\"completion\"]]\n",
    "    return hf_dataset\n",
    "\n",
    "train_dataset = train_dataset.map(fix_dataset_format)\n",
    "print(\"Sample record:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "# === 3) Model loading and optional QLoRA quantization ===\n",
    "# NOTE: Qwen3-VL classes may be provided under custom repo names (e.g. Qwen/Qwen3-VL-...)\n",
    "MODEL_NAME = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "\n",
    "# If you have bitsandbytes installed and want QLoRA, use BitsAndBytesConfig as in your snippet.\n",
    "use_qloRA = True\n",
    "\n",
    "if use_qloRA:\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        print(\"Attempting to load model with 4-bit quantization (QLoRA)...\")\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        # Many model implementations let you call from_pretrained with trust_remote_code=True\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", quantization_config=bnb_config, trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        print(\"QLoRA/4-bit load failed - falling back to normal fp16 load. Error:\\n\", e)\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# LoRA config\n",
    "from peft import LoraConfig\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"down_proj\",\"o_proj\",\"k_proj\",\"q_proj\",\"gate_proj\",\"up_proj\",\"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# === 4) Setup TRL SFT trainer ===\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "OUTPUT_DIR = \"SFT_VLA_Qwen3-VL-4B-Instruct-multimage-trl\"\n",
    "training_args = SFTConfig(\n",
    "    max_steps=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"adamw_8bit\",\n",
    "    max_length=None,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",  # set to trackio or wandb if available\n",
    "    push_to_hub=False,   # set True if you want to push adapters\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# View GPU info (if GPU present)\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(\"GPU available:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"No CUDA GPU detected. Training will be very slow or will fail for 4-bit loads.\")\n",
    "\n",
    "# === 5) Train (UNCOMMENT to run) ===\n",
    "# trainer_stats = trainer.train()\n",
    "# trainer.save_model(OUTPUT_DIR)\n",
    "# trainer.push_to_hub(dataset_name=OUTPUT_DIR)\n",
    "\n",
    "# === 6) Save adapter locally (after training) and later load for inference ===\n",
    "# For demonstration we'll assume the adapter is saved to OUTPUT_DIR locally.\n",
    "\n",
    "# === 7) Inference pipeline ===\n",
    "# This section demonstrates how to run the multi-image V+L assistant on frames extracted from a video.\n",
    "\n",
    "# Load model + adapter for inference (example):\n",
    "BASE_MODEL = MODEL_NAME\n",
    "ADAPTER_DIR = OUTPUT_DIR  # local path to adapter after training\n",
    "\n",
    "# If the model object above already exists with adapter loaded, you can skip re-loading.\n",
    "# Here we show a clean reload path.\n",
    "\n",
    "def load_inference_model(base_model: str, adapter_path: str = None, device: str = None):\n",
    "    \"\"\"Load base model and optionally merge/apply a PEFT adapter for inference.\n",
    "    Returns (model, processor, device).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Processor: contains image transforms + tokenizer\n",
    "    processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "    # Load base model (float16 if possible)\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "    if adapter_path is not None and os.path.exists(adapter_path):\n",
    "        try:\n",
    "            from peft import PeftModel\n",
    "            model = PeftModel.from_pretrained(model, adapter_path, device_map={\"\": device})\n",
    "            print(\"Loaded PEFT adapter from\", adapter_path)\n",
    "        except Exception as e:\n",
    "            print(\"Warning: failed to load adapter. Continuing with base model. Error:\\n\", e)\n",
    "\n",
    "    model.eval()\n",
    "    return model, processor, device\n",
    "\n",
    "\n",
    "# --- Frame extraction helper ---\n",
    "\n",
    "def extract_frames(video_path, output_image_folder, max_frames=8):\n",
    "    \"\"\"Extract `max_frames` evenly spaced frames from a video and save them to `output_image_folder`.\n",
    "    Returns a list of saved image paths.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_image_folder, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Error opening video: {video_path}\")\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames <= 0:\n",
    "        # fallback: try reading sequential frames until exhaustion\n",
    "        print(\"Warning: video reports 0 total frames. Attempting sequential extraction up to max_frames.\")\n",
    "        total_frames = max_frames\n",
    "\n",
    "    # choose frame indices evenly\n",
    "    frame_indices = [int(total_frames * i / max_frames) for i in range(max_frames)]\n",
    "    saved = []\n",
    "    for idx, frame_num in enumerate(tqdm(frame_indices, desc=\"Extracting frames\")):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Warning: could not read frame {frame_num} (idx {idx}). Skipping.\")\n",
    "            continue\n",
    "        # frame is BGR (cv2). Convert to RGB and save\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil = Image.fromarray(rgb)\n",
    "        save_path = os.path.join(output_image_folder, f\"frame_{idx+1:03d}.jpg\")\n",
    "        pil.save(save_path)\n",
    "        saved.append(save_path)\n",
    "\n",
    "    cap.release()\n",
    "    return saved\n",
    "\n",
    "\n",
    "# --- Preprocess frames for model ---\n",
    "\n",
    "def load_images_as_pil(image_paths):\n",
    "    \"\"\"Load image paths into a list of PIL images (RGB).\"\"\"\n",
    "    images = []\n",
    "    for p in image_paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# --- Compose prompt for multi-image V+L model ---\n",
    "PROMPT_SYSTEM = (\n",
    "    \"You are an autonomous driving vision-language assistant. Answer concisely and focus on the driving scene. \"\n",
    ")\n",
    "\n",
    "# Example prompt template: include context and instructions\n",
    "def build_prompt(question: str, extra_instructions: str = None):\n",
    "    \"\"\"Return the textual prompt to pass to the model along with images.\n",
    "    For many VL models the prompt is appended to the text field and processor assembles inputs.\n",
    "    \"\"\"\n",
    "    prompt = PROMPT_SYSTEM + \"\\n\\n\"\n",
    "    if extra_instructions:\n",
    "        prompt += extra_instructions + \"\\n\\n\"\n",
    "    prompt += f\"Question: {question}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# --- Run a single inference ---\n",
    "\n",
    "def run_inference_on_images(model, processor, device, pil_images, prompt, max_length=512, temperature=0.0):\n",
    "    \"\"\"Run the multimodal model. Returns decoded string.\n",
    "\n",
    "    Note: Exact input signature depends on the `processor` implementation. We use a generic\n",
    "    approach: processor(images=..., text=..., return_tensors='pt') and feed to model.generate.\n",
    "    \"\"\"\n",
    "    # Convert to tensors with processor\n",
    "    inputs = processor(images=pil_images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Move tensors to device\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.to(device)\n",
    "        elif isinstance(v, dict):\n",
    "            # nested (e.g. pixel_values)\n",
    "            for subk, subv in v.items():\n",
    "                if isinstance(subv, torch.Tensor):\n",
    "                    inputs[k][subk] = subv.to(device)\n",
    "\n",
    "    # Typical multimodal generate call (API may differ for Qwen3-VL implementations)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length, do_sample=False, temperature=temperature)\n",
    "\n",
    "    # decode generated ids with processor.tokenizer if present\n",
    "    tokenizer = getattr(processor, \"tokenizer\", None)\n",
    "    if tokenizer is None:\n",
    "        # fall back to model's tokenizer\n",
    "        tokenizer = getattr(model, \"get_tokenizer\", None)\n",
    "\n",
    "    if tokenizer is None:\n",
    "        # If we don't have a tokenizer object, return raw output tensor shape info\n",
    "        return str(outputs)\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # usually batch size is 1 -> return first element\n",
    "    return decoded[0]\n",
    "\n",
    "\n",
    "# --- High level wrapper that accepts a video, question and returns answer ---\n",
    "\n",
    "def answer_question_from_video(model, processor, device, video_path, question, frames_folder=\"/tmp/frames\", max_frames=6):\n",
    "    print(\"Extracting frames...\")\n",
    "    frames = extract_frames(video_path, frames_folder, max_frames=max_frames)\n",
    "    pil_images = load_images_as_pil(frames)\n",
    "    print(\"Running model inference...\")\n",
    "    answer = run_inference_on_images(model, processor, device, pil_images, question)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Example demonstration: load the model and run a quick inference on a short video file.\n",
    "    # Replace `/path/to/video.mp4` with your video on Colab (or mount Drive).\n",
    "    example_video = \"/path/to/video.mp4\"\n",
    "    if os.path.exists(example_video):\n",
    "        model_inf, proc, dev = load_inference_model(BASE_MODEL, ADAPTER_DIR)\n",
    "        question = \"Is there a pedestrian crossing the road in these frames? If yes, where?\"\n",
    "        answer = answer_question_from_video(model_inf, proc, dev, example_video, question, frames_folder=\"/tmp/frames\", max_frames=6)\n",
    "        print(\"Model answer:\\n\", answer)\n",
    "    else:\n",
    "        print(\"Example video not found. To try inference: set example_video to a real path and re-run.\")\n",
    "\n",
    "\n",
    "# === Helpful tips / next steps ===\n",
    "# - For large-scale evaluation create a dataset of (video, question, reference) and run batch inference.\n",
    "# - You can adapt build_prompt to include bounding-box requests or structured output tokens if you want\n",
    "#   the model to return JSON-like outputs for downstream parsing.\n",
    "# - When pushing adapters to the Hub, ensure you remove any large binaries from the repo, and\n",
    "#   provide a README explaining how to re-load the adapter with the base model.\n",
    "\n",
    "# End of notebook script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b3c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f5580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18a5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44c2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236bd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
